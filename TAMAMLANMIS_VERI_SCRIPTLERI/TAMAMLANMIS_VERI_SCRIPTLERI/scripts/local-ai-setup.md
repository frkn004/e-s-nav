# ðŸ¤– LOCAL AI MODEL KURULUMU VE EÄžÄ°TÄ°MÄ°

## ðŸŽ¯ **HEDEF**: API Maliyetlerinden Kurtulmak

### ðŸ’° **Maliyet KarÅŸÄ±laÅŸtÄ±rmasÄ±:**
- **OpenAI API**: $20-100/ay 
- **Local Model**: $0/ay (sadece elektrik)
- **Ä°lk kurulum**: $200-500 GPU (tek seferlik)

---

## ðŸš€ **AÅžAMA 1: OLLAMA KURULUMU (30 dakika)**

### 1.1 Ollama Ä°ndir ve Kur
```bash
# macOS iÃ§in
curl -fsSL https://ollama.ai/install.sh | sh

# Veya direkt indirme
# https://ollama.ai/download
```

### 1.2 Llama 3.1 Modeli Ä°ndir
```bash
# 8B model (16GB RAM yeterli)
ollama pull llama3.1:8b

# 70B model (64GB+ RAM gerekli)  
ollama pull llama3.1:70b

# TÃ¼rkÃ§e optimized (Ã¶nerilen)
ollama pull llama3.1:8b-instruct-q4_K_M
```

### 1.3 Test Et
```bash
ollama run llama3.1:8b "Ehliyet sÄ±navÄ±nda trafik kurallarÄ± nelerdir?"
```

---

## ðŸ§  **AÅžAMA 2: FINE-TUNING HAZIRLIÄžI**

### 2.1 Verilerimizi Kullan
Zaten hazÄ±rladÄ±ÄŸÄ±mÄ±z dosyalar:
- âœ… `ehliyet_finetuning.jsonl` (16 diyalog)
- âœ… `ehliyet_embedding_data.json` (8 bÃ¶lÃ¼m)
- âœ… `ehliyet_qa_dataset.json` (soru-cevap)

### 2.2 Llama Factory Kurulumu
```bash
# LLaMA-Factory (en iyi fine-tuning aracÄ±)
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -r requirements.txt
```

### 2.3 Dataset FormatÄ± DÃ¼zenle
```python
# convert_dataset.py
import json

# Ehliyet dataseti Llama Factory formatÄ±na Ã§evir
def convert_to_llama_format():
    with open('../data/ehliyet_finetuning.jsonl', 'r') as f:
        data = [json.loads(line) for line in f]
    
    llama_format = []
    for item in data:
        llama_format.append({
            "instruction": "Sen bir ehliyet eÄŸitimi uzmanÄ±sÄ±n.",
            "input": item["messages"][0]["content"],
            "output": item["messages"][1]["content"]
        })
    
    with open('ehliyet_dataset.json', 'w') as f:
        json.dump(llama_format, f, ensure_ascii=False, indent=2)

convert_to_llama_format()
```

---

## ðŸŽ“ **AÅžAMA 3: FINE-TUNING (2-6 saat)**

### 3.1 EÄŸitim Komutu
```bash
python src/train.py \
    --model_name_or_path llama3.1:8b \
    --do_train \
    --dataset ehliyet_dataset \
    --template llama3 \
    --finetuning_type lora \
    --lora_target q_proj,v_proj \
    --output_dir ./saves/ehliyet-llama \
    --overwrite_cache \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 5e-5 \
    --num_train_epochs 3.0 \
    --max_samples 1000 \
    --val_size 0.1 \
    --plot_loss \
    --fp16
```

### 3.2 LoRA Adapters Kullan (Ã–nerilen)
- **Avantajlar**: HÄ±zlÄ± eÄŸitim, az memory
- **Boyut**: 100MB (8B model iÃ§in)
- **SÃ¼re**: 2-4 saat

---

## ðŸ”„ **AÅžAMA 4: MODEL ENTEGRASYONU**

### 4.1 Ollama Modelfile OluÅŸtur
```dockerfile
# Modelfile
FROM llama3.1:8b

# Ehliyet uzmanÄ± kiÅŸiliÄŸi
SYSTEM """
Sen bir ehliyet eÄŸitimi uzmanÄ±sÄ±n. TÃ¼rkiye trafik kurallarÄ±, ilk yardÄ±m ve araÃ§ tekniÄŸi konularÄ±nda bilgilisin. 
Ã–ÄŸrencilere yardÄ±mcÄ± olurken:
- Net ve anlaÅŸÄ±lÄ±r aÃ§Ä±klamalar yap
- Ã–rneklerle destekle  
- SÄ±nav odaklÄ± cevaplar ver
- Motive edici ol
"""

# SÄ±caklÄ±k ayarlarÄ±
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
```

### 4.2 Custom Model OluÅŸtur
```bash
ollama create ehliyet-ai -f Modelfile
ollama run ehliyet-ai "Åžehir iÃ§i hÄ±z limiti nedir?"
```

---

## ðŸŒ **AÅžAMA 5: API SERVER KURULUMU**

### 5.1 Ollama API Server
```bash
# Ollama servisi baÅŸlat (arka planda)
ollama serve

# Test et
curl http://localhost:11434/api/generate -d '{
  "model": "ehliyet-ai",
  "prompt": "Emniyet kemeri neden takÄ±lÄ±r?",
  "stream": false
}'
```

### 5.2 Express.js Wrapper API
```javascript
// local-ai-server.js
const express = require('express');
const axios = require('axios');

const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  try {
    const { message } = req.body;
    
    const response = await axios.post('http://localhost:11434/api/generate', {
      model: 'ehliyet-ai',
      prompt: message,
      stream: false
    });

    res.json({
      success: true,
      message: response.data.response
    });
  } catch (error) {
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

app.listen(8000, () => {
  console.log('ðŸ¤– Local AI Server running on port 8000');
});
```

---

## ðŸš€ **AÅžAMA 6: PRODUCTION HAZIRLIK**

### 6.1 Docker Container
```dockerfile
# Dockerfile
FROM ollama/ollama:latest

# Model dosyalarÄ±nÄ± kopyala
COPY ./models /root/.ollama/models
COPY ./Modelfile /app/

# Servisi baÅŸlat
EXPOSE 11434
CMD ["ollama", "serve"]
```

### 6.2 Nginx Reverse Proxy
```nginx
# /etc/nginx/sites-available/ai-api
server {
    listen 80;
    server_name ai.ehliyet-saas.com;
    
    location / {
        proxy_pass http://localhost:11434;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

---

## ðŸ“Š **PERFORMANS VE SCALABILITY**

### Hardware Gereksinimleri:
- **Minimum**: 16GB RAM, RTX 3060
- **Optimal**: 32GB RAM, RTX 4080  
- **Enterprise**: 64GB RAM, RTX 4090

### Model BoyutlarÄ±:
- **7B-8B**: 4-8GB VRAM
- **13B**: 8-16GB VRAM
- **70B**: 40-80GB VRAM

### HÄ±z KarÅŸÄ±laÅŸtÄ±rmasÄ±:
- **Local 8B**: ~50 token/saniye
- **OpenAI GPT-4**: ~20 token/saniye
- **Latency**: Local %90 daha hÄ±zlÄ±

---

## ðŸ’¡ **EK Ã–PTÄ°MÄ°ZASYONLAR**

### 1. Quantization (Model SÄ±kÄ±ÅŸtÄ±rma)
```bash
# Q4_K_M format (boyut yarÄ±ya iner)
ollama pull llama3.1:8b-instruct-q4_K_M
```

### 2. RAG Entegrasyonu
```python
# vector_store.py
from langchain.vectorstores import Chroma
from langchain.embeddings import OllamaEmbeddings

embeddings = OllamaEmbeddings(model="llama3.1:8b")
vectorstore = Chroma.from_documents(
    documents=ehliyet_docs,
    embedding=embeddings
)
```

### 3. Multi-GPU Setup
```python
# distributed_inference.py
import torch
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "ehliyet-llama",
    device_map="auto",
    torch_dtype=torch.float16
)
```

---

## ðŸŽ¯ **ROI HESABI (YatÄ±rÄ±m DÃ¶nÃ¼ÅŸÃ¼)**

### Maliyet Analizi:
- **Hardware**: $1000 (RTX 4080)
- **Elektrik**: $20/ay
- **BakÄ±m**: $0

### Tasarruf:
- **OpenAI API**: $100/ay Ã— 12 = $1200/yÄ±l
- **Break-even**: 10 ay
- **2 yÄ±l sonrasÄ±**: %100 tasarruf

---

## âœ… **SONUÃ‡: MÃœKEMMEl PLAN!**

1. **1. Hafta**: Ollama kurulumu + test
2. **2. Hafta**: Fine-tuning + Ã¶zelleÅŸtirme  
3. **3. Hafta**: Production deploy
4. **4. Hafta**: Optimizasyon

**Toplam maliyet**: $1000 hardware
**YÄ±llÄ±k tasarruf**: $1200+
**Bonus**: %90 daha hÄ±zlÄ± yanÄ±t!

Bu strateji ile hem maliyet dÃ¼ÅŸer hem de performans artar! ðŸš€ 